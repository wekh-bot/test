import urllib.request
import re
import base64
import json
import os
from datetime import datetime, timezone, timedelta

emoji_to_country = {
    'ğŸ‡¨ğŸ‡³': 'CN', 'ğŸ‡ºğŸ‡¸': 'US', 'ğŸ‡¸ğŸ‡¬': 'SG', 'ğŸ‡¯ğŸ‡µ': 'JP',
    'ğŸ‡°ğŸ‡·': 'KR', 'ğŸ‡¹ğŸ‡¼': 'TW', 'ğŸ‡­ğŸ‡°': 'HK'
}

country_code_to_name = {
    'CN': 'ä¸­å›½', 'US': 'ç¾å›½', 'JP': 'æ—¥æœ¬', 'HK': 'é¦™æ¸¯', 'æœªçŸ¥': 'æœªçŸ¥'
}

TARGET_COUNTRIES = ["US", "JP", "HK"]  # åªè¦è¿™ä¸‰ä¸ªå›½å®¶
MAX_PER_COUNTRY = 10  # æ¯ä¸ªå›½å®¶æœ€å¤š10ä¸ªèŠ‚ç‚¹


class BsbbCrawler:
    def __init__(self):
        self.url = "https://www.bsbb.cc/V2RAY.txt"
        self.nodes = []

    def fetch_node_data(self):
        """è·å–èŠ‚ç‚¹åŸå§‹æ•°æ®"""
        try:
            print("æ­£åœ¨è·å–èŠ‚ç‚¹æ•°æ®...")
            response = urllib.è¯·æ±‚.urlopen(self.url, timeout=15)
            data = response.read().decode("utf-8")
            return data.strip().split("\n")
        except Exception as e:
            print(f"è·å–èŠ‚ç‚¹æ•°æ®å¤±è´¥: {e}")
            return []

    def parse_node(self, line):
        """è§£æå•ä¸ªèŠ‚ç‚¹"""
        protocol_match = re.match(r'([^:]+)://', line)
        if not protocol_match:
            return None
        protocol = protocol_match.group(1).lower()
        if protocol != "vless" and protocol != "vmess" and protocol != "trojan":
            return None  # é ws èŠ‚ç‚¹åè®®ä¸å¤„ç†

        # wsåè®®è¯†åˆ«ï¼ˆåŒ…å«wsã€wssï¼‰
        if "ws" not in line.lower():
            return None

        remark_match = re.search(r'#(.+)$', line)
        remark = remark_match.group(1) if remark_match else ""
        emoji_match = re.search(r'^([\U0001F1E6-\U0001F1FF]{2})', remark)
        latency_match = re.search(r'(\d+)ms$', remark)
        country = emoji_to_country.get(emoji_match.group(1), "æœªçŸ¥") if emoji_match else "æœªçŸ¥"
        latency = int(latency_match.group(1)) if latency_match else 9999

        return {"raw": line, "country": country, "latency": latency, "protocol": protocol}

    def crawl(self):
        """æ‰§è¡Œçˆ¬å–"""
        raw_lines = self.fetch_node_data()
        for line in raw_lines:
            if line.strip():
                node = self.parse_node(line.strip())
                if node:
                    self.nodes.append(node)
        print(f"å…±çˆ¬å–åˆ° {len(self.nodes)} ä¸ªèŠ‚ç‚¹")
        return self.nodes

    def filter_nodes(self):
        """ç­›é€‰ç¬¦åˆè¦æ±‚çš„èŠ‚ç‚¹"""
        filtered = []
        for country in TARGET_COUNTRIES:
            nodes = [n for n in self.nodes if n["country"] == country]
            nodes = sorted(nodes, key=lambda x: x["latency"])[:MAX_PER_COUNTRY]
            filtered.extend(nodes)
            print(f"{country_code_to_name[country]} ä¿ç•™ {len(nodes)} ä¸ª ws èŠ‚ç‚¹")
        self.nodes = filtered
        print(f"ç­›é€‰åå…± {len(filtered)} ä¸ªèŠ‚ç‚¹")

    def save_to_files(self):
        """ä¿å­˜ config.txt å’Œæ›´æ–° README.md"""
        workspace = os.getenv("GITHUB_WORKSPACE", os.path.abspath("../../"))

        # ä¿å­˜ config.txt
        config_path = os.path.join(workspace, "config.txt")
        with open(config_path, "w", encoding="utf-8") as f:
            for node in self.nodes:
                f.write(node["raw"] + "\n")
        print(f"âœ… å·²ä¿å­˜ {len(self.nodes)} ä¸ªèŠ‚ç‚¹åˆ° {config_path}")

        # æ›´æ–° README.mdï¼Œä»…æ›´æ–°æ—¶é—´
        readme_path = os.path.join(workspace, "README.md")
        china_tz = timezone(timedelta(hours=8))
        now = datetime.now(china_tz).strftime("%Y-%m-%d %H:%M:%S")
        with open(readme_path, "w", encoding="utf-8") as f:
            f.write(f"# æ›´æ–°æ—¶é—´\n\næœ€åæ›´æ–°: {now}\n")
        print(f"âœ… README.md æ›´æ–°æ—¶é—´å·²å†™å…¥: {now}")


if __name__ == "__main__":
    crawler = BsbbCrawler()
    if crawler.crawl():
        crawler.filter_nodes()
        crawler.save_to_files()
