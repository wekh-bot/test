import urllib.request
import re
import base64
import json
import os
from datetime import datetime, timezone, timedelta

# emojiåˆ°å›½å®¶ä»£ç çš„æ˜ å°„
emoji_to_country = {
    'ğŸ‡¨ğŸ‡³': 'CN', 'ğŸ‡ºğŸ‡¸': 'US', 'ğŸ‡¸ğŸ‡¬': 'SG', 'ğŸ‡©ğŸ‡ª': 'DE', 'ğŸ‡¬ğŸ‡§': 'GB',
    'ğŸ‡¯ğŸ‡µ': 'JP', 'ğŸ‡°ğŸ‡·': 'KR', 'ğŸ‡«ğŸ‡·': 'FR', 'ğŸ‡·ğŸ‡º': 'RU', 'ğŸ‡®ğŸ‡³': 'IN',
    'ğŸ‡§ğŸ‡·': 'BR', 'ğŸ‡¨ğŸ‡¦': 'CA', 'ğŸ‡¦ğŸ‡º': 'AU', 'ğŸ‡³ğŸ‡±': 'NL', 'ğŸ‡®ğŸ‡©': 'ID',
    'ğŸ‡¹ğŸ‡­': 'TH', 'ğŸ‡»ğŸ‡³': 'VN', 'ğŸ‡µğŸ‡­': 'PH', 'ğŸ‡²ğŸ‡¾': 'MY', 'ğŸ‡¹ğŸ‡¼': 'TW',
    'ğŸ‡­ğŸ‡°': 'HK', 'ğŸ‡²ğŸ‡´': 'MO', 'ğŸ‡¨ğŸ‡¼': 'CW', 'ğŸ‡ªğŸ‡¸': 'ES', 'ğŸ‡¹ğŸ‡·': 'TR',
    'ğŸ‡³ğŸ‡´': 'NO', 'ğŸ‡ºğŸ‡¦': 'UA', 'ğŸ‡±ğŸ‡»': 'LV', 'ğŸ‡°ğŸ‡­': 'KH', 'ğŸ‡¸ğŸ‡ª': 'SE',
    'ğŸ‡«ğŸ‡®': 'FI', 'ğŸ‡·ğŸ‡´': 'RO', 'ğŸ‡§ğŸ‡ª': 'BE'
}

# å›½å®¶ä»£ç åˆ°ä¸­æ–‡åç§°çš„æ˜ å°„
country_code_to_name = {
    'CN': 'ä¸­å›½', 'US': 'ç¾å›½', 'SG': 'æ–°åŠ å¡', 'DE': 'å¾·å›½', 'GB': 'è‹±å›½',
    'JP': 'æ—¥æœ¬', 'KR': 'éŸ©å›½', 'FR': 'æ³•å›½', 'RU': 'ä¿„ç½—æ–¯', 'IN': 'å°åº¦',
    'BR': 'å·´è¥¿', 'CA': 'åŠ æ‹¿å¤§', 'AU': 'æ¾³å¤§åˆ©äºš', 'NL': 'è·å…°', 'ID': 'å°åº¦å°¼è¥¿äºš',
    'TH': 'æ³°å›½', 'VN': 'è¶Šå—', 'PH': 'è²å¾‹å®¾', 'MY': 'é©¬æ¥è¥¿äºš', 'TW': 'å°æ¹¾',
    'HK': 'é¦™æ¸¯', 'MO': 'æ¾³é—¨', 'CW': 'åº“æ‹‰ç´¢', 'ES': 'è¥¿ç­ç‰™', 'TR': 'åœŸè€³å…¶',
    'NO': 'æŒªå¨', 'UA': 'ä¹Œå…‹å…°', 'LV': 'æ‹‰è„±ç»´äºš', 'KH': 'æŸ¬åŸ”å¯¨', 'SE': 'ç‘å…¸',
    'FI': 'èŠ¬å…°', 'RO': 'ç½—é©¬å°¼äºš', 'BE': 'æ¯”åˆ©æ—¶', 'æœªçŸ¥': 'æœªçŸ¥'
}

class BsbbCrawler:
    def __init__(self):
        self.base_url = "https://www.bsbb.cc"
        self.node_file_url = f"{self.base_url}/V2RAY.txt"
        self.nodes = []

    def fetch_node_data(self):
        """è·å–èŠ‚ç‚¹æ•°æ®"""
        try:
            response = urllib.request.urlopen(self.node_file_url, timeout=10)
            data = response.read().decode('utf-8')
            return data.strip().split('\n')
        except Exception as e:
            print(f"è·å–èŠ‚ç‚¹æ•°æ®æ—¶å‡ºé”™: {e}")
            return []

    def parse_node(self, node_line):
        """è§£æå•ä¸ªèŠ‚ç‚¹ä¿¡æ¯"""
        # æå–åè®®ç±»å‹
        protocol_match = re.match(r'([^:]+)://', node_line)
        if not protocol_match:
            return None
            
        protocol = protocol_match.group(1).lower()
        
        # æå–å¤‡æ³¨ä¿¡æ¯ï¼ˆåŒ…å«å›½å®¶å’Œå»¶è¿Ÿï¼‰
        remark_match = re.search(r'#(.+)$', node_line)
        remark = remark_match.group(1) if remark_match else ""
        
        # æå–å›½å®¶ä»£ç å’Œå»¶è¿Ÿ
        # ä»å¤‡æ³¨ä¸­æå–å›½å®¶ä»£ç ï¼ˆä¾‹å¦‚ï¼šğŸ‡ºğŸ‡¸ www.bsbb.cc vless-US 87msï¼‰
        country_emoji_match = re.search(r'^([\U0001F1E6-\U0001F1FF]{2})', remark)
        country_code_match = re.search(r'([A-Z]{2})\s*www\.bsbb\.cc\s*[a-zA-Z]+-([A-Z]{2})', remark)
        latency_match = re.search(r'(\d+)ms$', remark)
        
        # ä¼˜å…ˆä½¿ç”¨emojiä¸­çš„å›½å®¶ä»£ç ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨åŸæ¥çš„æå–æ–¹å¼
        if country_emoji_match:
            country_emoji = country_emoji_match.group(1)
            country_code = emoji_to_country.get(country_emoji, "æœªçŸ¥")
        elif country_code_match:
            country_code = country_code_match.group(2)
        else:
            country_code = "æœªçŸ¥"
            
        latency = int(latency_match.group(1)) if latency_match else None
        
        # æå–ä¸»æœºå’Œç«¯å£
        host, port = self.extract_host_port(node_line, protocol)
        
        return {
            "protocol": protocol,
            "country_code": country_code,
            "latency": latency,
            "host": host,
            "port": port,
            "raw": node_line
        }

    def extract_host_port(self, node_line, protocol):
        """ä»èŠ‚ç‚¹é“¾æ¥ä¸­æå–ä¸»æœºå’Œç«¯å£"""
        try:
            if protocol == "vmess":
                # VmessèŠ‚ç‚¹éœ€è¦base64è§£ç 
                encoded_data = node_line[8:]  # å»æ‰"vmess://"
                # æ·»åŠ ç¼ºå°‘çš„å¡«å……å­—ç¬¦
                missing_padding = len(encoded_data) % 4
                if missing_padding:
                    encoded_data += '=' * (4 - missing_padding)
                
                # å¤„ç†éASCIIå­—ç¬¦
                decoded_data = base64.b64decode(encoded_data.encode('ascii')).decode('utf-8')
                data = json.loads(decoded_data)
                host = data.get("add", "")
                port = data.get("port", "")
                return host, port
            else:
                # å…¶ä»–åè®®ç±»å‹
                if "?" in node_line:
                    url_part = node_line.split("?")[0]
                else:
                    url_part = node_line.split("#")[0]
                
                host_port = url_part.split("@")[-1].split(":")
                host = host_port[0] if len(host_port) > 0 else ""
                port = host_port[1] if len(host_port) > 1 else ""
                return host, port
        except Exception as e:
            # ä¸æ˜¾ç¤ºé”™è¯¯ä¿¡æ¯ï¼Œé¿å…å¹²æ‰°
            return "", ""

    def crawl(self):
        """æ‰§è¡Œçˆ¬å–ä»»åŠ¡"""
        print("å¼€å§‹çˆ¬å– www.bsbb.cc èŠ‚ç‚¹ä¿¡æ¯...")
        node_lines = self.fetch_node_data()
        
        if not node_lines:
            print("æœªèƒ½è·å–åˆ°èŠ‚ç‚¹æ•°æ®")
            return
            
        for line in node_lines:
            if line.strip():
                node_info = self.parse_node(line.strip())
                if node_info:
                    self.nodes.append(node_info)
        
        print(f"çˆ¬å–å®Œæˆï¼Œå…±è·å–åˆ° {len(self.nodes)} ä¸ªèŠ‚ç‚¹ä¿¡æ¯")
        print(f"æ€»å…±å¤„ç†äº† {len(node_lines)} è¡Œæ•°æ®")
        return self.nodes

    def analyze_nodes(self):
        """åˆ†æèŠ‚ç‚¹ä¿¡æ¯"""
        if not self.nodes:
            print("æ²¡æœ‰èŠ‚ç‚¹æ•°æ®å¯ä¾›åˆ†æ")
            return
            
        # ç»Ÿè®¡æ€»èŠ‚ç‚¹æ•°
        total_nodes = len(self.nodes)
        
        # ç»Ÿè®¡å»é‡åçš„èŠ‚ç‚¹æ•°
        unique_nodes = len(set(node['raw'] for node in self.nodes))
        duplicate_nodes = total_nodes - unique_nodes
        
        # æŒ‰å›½å®¶ç»Ÿè®¡èŠ‚ç‚¹æ•°
        country_count = {}
        for node in self.nodes:
            country = node['country_code']
            country_count[country] = country_count.get(country, 0) + 1
            
        # æŒ‰åè®®ç±»å‹ç»Ÿè®¡èŠ‚ç‚¹æ•°
        protocol_count = {}
        for node in self.nodes:
            protocol = node['protocol']
            protocol_count[protocol] = protocol_count.get(protocol, 0) + 1
            
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\nèŠ‚ç‚¹ç»Ÿè®¡ä¿¡æ¯:")
        print(f"æ€»èŠ‚ç‚¹æ•°: {total_nodes}")
        print(f"é‡å¤èŠ‚ç‚¹æ•°: {duplicate_nodes}")
        print(f"å»é‡åèŠ‚ç‚¹æ•°: {unique_nodes}")
        
        print(f"\næŒ‰å›½å®¶åŒºåŸŸç»Ÿè®¡:")
        for country, count in sorted(country_count.items()):
            country_name = country_code_to_name.get(country, country)
            print(f"{country_name}: {count} ä¸ªèŠ‚ç‚¹")
            
        print(f"\næŒ‰åè®®ç±»å‹ç»Ÿè®¡:")
        for protocol, count in sorted(protocol_count.items()):
            print(f"{protocol}: {count} ä¸ªèŠ‚ç‚¹")
            
        return {
            'total': total_nodes,
            'unique': unique_nodes,
            'duplicates': duplicate_nodes,
            'countries': country_count,
            'protocols': protocol_count
        }

    def save_to_file(self, filename="nodes.txt"):
        """ä¿å­˜èŠ‚ç‚¹ä¿¡æ¯åˆ°æ–‡ä»¶ï¼ˆå»é‡åï¼‰"""
        # å»é‡èŠ‚ç‚¹
        unique_nodes = list(set(node['raw'] for node in self.nodes))
        
        with open(filename, "w", encoding="utf-8") as f:
            for node_raw in unique_nodes:
                f.write(f"{node_raw}\n")
        print(f"å»é‡åçš„èŠ‚ç‚¹ä¿¡æ¯å·²ä¿å­˜åˆ° {filename}ï¼Œå…± {len(unique_nodes)} ä¸ªèŠ‚ç‚¹")

    def update_readme(self, analysis_result):
        """æ›´æ–° README.md æ–‡ä»¶"""
        readme_path = "../README.md"
        
        # ä½¿ç”¨ä¸­å›½æ—¶åŒº
        china_tz = timezone(timedelta(hours=8))
        now = datetime.now(china_tz)
        
        # åˆ›å»º README.md å†…å®¹
        readme_content = "# çˆ¬è™«ç»“æœç»Ÿè®¡\n\n"
        readme_content += f"æœ€åæ›´æ–°æ—¶é—´: {now.strftime('%Y-%m-%d %H:%M:%S')}\n\n"
        readme_content += f"æ€»èŠ‚ç‚¹æ•°: {analysis_result['total']}\n\n"
        readme_content += f"å»é‡åèŠ‚ç‚¹æ•°: {analysis_result['unique']}\n\n"
        readme_content += f"é‡å¤èŠ‚ç‚¹æ•°: {analysis_result['duplicates']}\n\n"
        
        readme_content += "## æŒ‰å›½å®¶åŒºåŸŸç»Ÿè®¡\n\n"
        for country, count in sorted(analysis_result['countries'].items()):
            country_name = country_code_to_name.get(country, country)
            readme_content += f"- {country_name}: {count} ä¸ªèŠ‚ç‚¹\n"
        
        readme_content += "\n## æŒ‰åè®®ç±»å‹ç»Ÿè®¡\n\n"
        for protocol, count in sorted(analysis_result['protocols'].items()):
            readme_content += f"- {protocol}: {count} ä¸ªèŠ‚ç‚¹\n"
        
        # å†™å…¥ README.md æ–‡ä»¶
        with open(readme_path, "w", encoding="utf-8") as f:
            f.write(readme_content)
        
        print(f"README.md å·²æ›´æ–°")

if __name__ == "__main__":
    crawler = BsbbCrawler()
    nodes = crawler.crawl()
    if nodes:
        # åˆ†æèŠ‚ç‚¹ä¿¡æ¯
        analysis_result = crawler.analyze_nodes()
        # ä¿å­˜å»é‡åçš„èŠ‚ç‚¹ä¿¡æ¯
        crawler.save_to_file()
        # æ›´æ–° README.md
        crawler.update_readme(analysis_result)
