import urllib.request
import re
import base64
import json
import os

emoji_to_country = {
    'ğŸ‡¨ğŸ‡³': 'CN', 'ğŸ‡ºğŸ‡¸': 'US', 'ğŸ‡¸ğŸ‡¬': 'SG', 'ğŸ‡¯ğŸ‡µ': 'JP',
    'ğŸ‡°ğŸ‡·': 'KR', 'ğŸ‡¹ğŸ‡¼': 'TW', 'ğŸ‡­ğŸ‡°': 'HK'
}

country_code_to_name = {
    'CN': 'ä¸­å›½', 'US': 'ç¾å›½', 'SG': 'æ–°åŠ å¡', 'JP': 'æ—¥æœ¬',
    'KR': 'éŸ©å›½', 'TW': 'å°æ¹¾', 'HK': 'é¦™æ¸¯', 'æœªçŸ¥': 'æœªçŸ¥'
}

TARGET_COUNTRIES = ["US", "TW", "HK", "JP", "KR", "SG"]

class BsbbCrawler:
    def __init__(self):
        self.base_url = "https://www.bsbb.cc/V2RAY.txt"
        self.nodes = []

    def fetch_node_data(self):
        try:
            print("æ­£åœ¨è·å–èŠ‚ç‚¹æ•°æ®...")
            with urllib.è¯·æ±‚.urlopen(self.base_url, timeout=15) as res:
                return res.read().decode('utf-8').strip().split('\n')
        except Exception as e:
            print(f"âŒ è·å–èŠ‚ç‚¹å¤±è´¥: {e}")
            return []

    def parse_node(self, line):
        match = re.match(r'([^:]+)://', line)
        if not match:
            return None
        protocol = match.group(1).lower()
        remark = re.search(r'#(.+)$', line)
        remark = remark.group(1) if remark else ""
        flag = re.search(r'^([\U0001F1E6-\U0001F1FF]{2})', remark)
        latency = re.search(r'(\d+)ms$', remark)
        country = emoji_to_country.get(flag.group(1), "æœªçŸ¥") if flag else "æœªçŸ¥"
        delay = int(latency.group(1)) if latency else 9999
        return {"raw": line, "country": country, "delay": delay, "protocol": protocol}

    def crawl(self):
        data = self.fetch_node_data()
        for line in data:
            if line.strip():
                node = self.parse_node(line.strip())
                if node:
                    self.nodes.append(node)
        print(f"âœ… è·å–åˆ° {len(self.nodes)} ä¸ªèŠ‚ç‚¹")
        return self.nodes

    def filter_nodes(self):
        result = []
        for c in TARGET_COUNTRIES:
            nodes = [n for n in self.nodes if n["country"] == c]
            nodes = sorted(nodes, key=lambda x: x["delay"])[:5]
            result += nodes
            print(f"{country_code_to_name[c]} ä¿ç•™ {len(nodes)} ä¸ªèŠ‚ç‚¹")
        self.nodes = result

    def save_to_root(self):
        """å¼ºåˆ¶ä¿å­˜åˆ°ä»“åº“æ ¹ç›®å½•"""
        root_path = os.getenv("GITHUB_WORKSPACE", os.path.abspath("../../"))
        save_path = os.path.join(root_path, "v2ray.txt")

        with open(save_path, "w", encoding="utf-8") as f:
            for node in self.nodes:
                f.write(f"{node['raw']}\n")

        print(f"âœ… å·²ä¿å­˜ {len(self.nodes)} ä¸ªèŠ‚ç‚¹åˆ° {save_path}")

if __name__ == "__main__":
    crawler = BsbbCrawler()
    if crawler.crawl():
        crawler.filter_nodes()
        crawler.save_to_root()
