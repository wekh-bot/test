import urllib.request
import re
import base64
import json
from datetime import datetime, timezone, timedelta

# emoji åˆ°å›½å®¶ä»£ç æ˜ å°„
emoji_to_country = {
    'ğŸ‡¨ğŸ‡³': 'CN', 'ğŸ‡ºğŸ‡¸': 'US', 'ğŸ‡¸ğŸ‡¬': 'SG', 'ğŸ‡©ğŸ‡ª': 'DE', 'ğŸ‡¬ğŸ‡§': 'GB',
    'ğŸ‡¯ğŸ‡µ': 'JP', 'ğŸ‡°ğŸ‡·': 'KR', 'ğŸ‡«ğŸ‡·': 'FR', 'ğŸ‡·ğŸ‡º': 'RU', 'ğŸ‡®ğŸ‡³': 'IN',
    'ğŸ‡§ğŸ‡·': 'BR', 'ğŸ‡¨ğŸ‡¦': 'CA', 'ğŸ‡¦ğŸ‡º': 'AU', 'ğŸ‡³ğŸ‡±': 'NL', 'ğŸ‡®ğŸ‡©': 'ID',
    'ğŸ‡¹ğŸ‡­': 'TH', 'ğŸ‡»ğŸ‡³': 'VN', 'ğŸ‡µğŸ‡­': 'PH', 'ğŸ‡²ğŸ‡¾': 'MY', 'ğŸ‡¹ğŸ‡¼': 'TW',
    'ğŸ‡­ğŸ‡°': 'HK', 'ğŸ‡²ğŸ‡´': 'MO', 'ğŸ‡¨ğŸ‡¼': 'CW', 'ğŸ‡ªğŸ‡¸': 'ES', 'ğŸ‡¹ğŸ‡·': 'TR',
    'ğŸ‡³ğŸ‡´': 'NO', 'ğŸ‡ºğŸ‡¦': 'UA', 'ğŸ‡±ğŸ‡»': 'LV', 'ğŸ‡°ğŸ‡­': 'KH', 'ğŸ‡¸ğŸ‡ª': 'SE',
    'ğŸ‡«ğŸ‡®': 'FI', 'ğŸ‡·ğŸ‡´': 'RO', 'ğŸ‡§ğŸ‡ª': 'BE'
}

# å›½å®¶ä»£ç åˆ°ä¸­æ–‡åæ˜ å°„
country_code_to_name = {
    'CN': 'ä¸­å›½', 'US': 'ç¾å›½', 'SG': 'æ–°åŠ å¡', 'JP': 'æ—¥æœ¬',
    'KR': 'éŸ©å›½', 'TW': 'å°æ¹¾', 'HK': 'é¦™æ¸¯', 'æœªçŸ¥': 'æœªçŸ¥'
}

# âœ… ä»…ä¿ç•™ä»¥ä¸‹å›½å®¶
TARGET_COUNTRIES = ["US", "TW", "HK", "JP", "KR", "SG"]

class BsbbCrawler:
    def __init__(self):
        self.base_url = "https://www.bsbb.cc"
        self.node_file_url = f"{self.base_url}/V2RAY.txt"
        self.nodes = []

    def fetch_node_data(self):
        """è·å–èŠ‚ç‚¹æ•°æ®"""
        try:
            response = urllib.request.urlopen(self.node_file_url, timeout=15)
            data = response.read().decode('utf-8')
            return data.strip().split('\n')
        except Exception as e:
            print(f"è·å–èŠ‚ç‚¹æ•°æ®æ—¶å‡ºé”™: {e}")
            return []

    def parse_node(self, node_line):
        """è§£æå•ä¸ªèŠ‚ç‚¹"""
        protocol_match = re.match(r'([^:]+)://', node_line)
        if not protocol_match:
            return None
        protocol = protocol_match.group(1).lower()
        remark_match = re.search(r'#(.+)$', node_line)
        remark = remark_match.group(1) if remark_match else ""

        # æå– emoji å›½å®¶
        country_emoji_match = re.search(r'^([\U0001F1E6-\U0001F1FF]{2})', remark)
        latency_match = re.search(r'(\d+)ms$', remark)

        if country_emoji_match:
            country_emoji = country_emoji_match.group(1)
            country_code = emoji_to_country.get(country_emoji, "æœªçŸ¥")
        else:
            country_code = "æœªçŸ¥"

        latency = int(latency_match.group(1)) if latency_match else 9999
        host, port = self.extract_host_port(node_line, protocol)
        return {
            "protocol": protocol,
            "country_code": country_code,
            "latency": latency,
            "host": host,
            "port": port,
            "raw": node_line
        }

    def extract_host_port(self, node_line, protocol):
        """æå–ä¸»æœºç«¯å£"""
        try:
            if protocol == "vmess":
                encoded_data = node_line[8:]
                missing_padding = len(encoded_data) % 4
                if missing_padding:
                    encoded_data += '=' * (4 - missing_padding)
                decoded_data = base64.b64decode(encoded_data.encode('ascii')).decode('utf-8')
                data = json.loads(decoded_data)
                return data.get("add", ""), data.get("port", "")
            else:
                url_part = node_line.split("?")[0].split("#")[0]
                host_port = url_part.split("@")[-1].split(":")
                host = host_port[0] if len(host_port) > 0 else ""
                port = host_port[1] if len(host_port) > 1 else ""
                return host, port
        except Exception:
            return "", ""

    def crawl(self):
        """çˆ¬å–èŠ‚ç‚¹"""
        print("å¼€å§‹çˆ¬å– www.bsbb.cc èŠ‚ç‚¹...")
        node_lines = self.fetch_node_data()
        if not node_lines:
            print("æœªè·å–åˆ°èŠ‚ç‚¹æ•°æ®")
            return
        for line in node_lines:
            if line.strip():
                info = self.parse_node(line.strip())
                if info:
                    self.nodes.append(info)
        print(f"çˆ¬å–å®Œæˆï¼Œå…± {len(self.nodes)} æ¡èŠ‚ç‚¹")
        return self.nodes

    def filter_nodes(self):
        """ç­›é€‰å»¶è¿Ÿæœ€ä½çš„èŠ‚ç‚¹"""
        # æŒ‰å›½å®¶åˆ†ç»„
        grouped = {country: [] for country in TARGET_COUNTRIES}
        for node in self.nodes:
            if node["country_code"] in TARGET_COUNTRIES:
                grouped[node["country_code"]].append(node)

        # æ¯ä¸ªå›½å®¶æŒ‰å»¶è¿Ÿæ’åºï¼Œå–å‰5
        filtered = []
        for country, nodes in grouped.items():
            nodes_sorted = sorted(nodes, key=lambda x: x["latency"])
            top_nodes = nodes_sorted[:5]
            filtered.extend(top_nodes)
            print(f"{country_code_to_name[country]} ä¿ç•™ {len(top_nodes)} ä¸ªèŠ‚ç‚¹")

        self.nodes = filtered
        print(f"ç­›é€‰åæ€»è®¡ {len(filtered)} ä¸ªèŠ‚ç‚¹")

    def save_to_file(self, filename="v2ray.txt"):
        """ä¿å­˜ç»“æœåˆ°ä»“åº“æ ¹ç›®å½•"""
        unique_nodes = list(dict.fromkeys(node['raw'] for node in self.nodes))
        with open(filename, "w", encoding="utf-8") as f:
            for node_raw in unique_nodes:
                f.write(f"{node_raw}\n")
        print(f"âœ… å·²ä¿å­˜ {len(unique_nodes)} ä¸ªèŠ‚ç‚¹åˆ° {filename}")

if __name__ == "__main__":
    crawler = BsbbCrawler()
    nodes = crawler.crawl()
    if nodes:
        crawler.filter_nodes()
        crawler.save_to_file("v2ray.txt")  # ä¿å­˜åˆ°ä»“åº“æ ¹ç›®å½•
